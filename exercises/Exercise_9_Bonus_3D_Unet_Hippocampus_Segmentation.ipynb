{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4MS: Hippocampus U-Net Segmentation with Pytorch Lightning + MONAI",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U7pGt29nuLj"
      },
      "source": [
        "# Hippocampus U-Net Segmentation with Pytorch Lightning + MONAI\n",
        "\n",
        "In this exercise we bring together all of what we have learned during the course. Instead of building things from scratch for learning the basics as we did during the previous exercises, we now apply convenient out of the box python modules that have best practice implementations of the state of the art of training and models for quick prototyping!\n",
        "\n",
        "We will be using these tools:\n",
        "\n",
        "* **PyTorch** as the deep learning framework: https://pytorch.org/\n",
        "* **Medical Segmentation Decathlon** for the hippocampus dataset: http://medicaldecathlon.com\n",
        "*   **Medical Open Network for AI (MONAI)** for transforms, downloading the data and models: https://docs.monai.io/en/latest/index.html\n",
        "*  **Pytorch Lightning** for dataloading and training: https://pytorchlightning.ai/\n",
        "\n",
        "\n",
        "![Hippocampus](https://upload.wikimedia.org/wikipedia/commons/9/99/Hippocampus.gif)\n",
        "\n",
        "\n",
        "Have fun!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2kX-3l8nbsV"
      },
      "source": [
        "## Check for GPU Runtime\n",
        "\n",
        "By default Google Colab runs without a GPU - we need one!\n",
        "Go to the menu and change it there:\n",
        "\n",
        "*   German: Laufzeit > Laufzeittyp aendern > Hardwarebeschleuniger = GPU\n",
        "*   English: Runtime > Change runtime type > Hardware accelerator = GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x716EWlUnXIp"
      },
      "source": [
        "import torch\n",
        "assert torch.cuda.device_count(), 'This exercise is a lot faster with a GPU - thanks gamers'\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UilR9pQr581H"
      },
      "source": [
        "## Hippocampus Segmentation Dataset\n",
        "\n",
        "For this example we will be using the Hippocampus Segmentation Challenge data from the Medical Segmentation Decathlon: \n",
        "\n",
        "http://medicaldecathlon.com/#tasks\n",
        "\n",
        "To make our live easier we will be using some methods from the Medical Open Network for AI (MONAI) module:\n",
        "\n",
        "https://docs.monai.io/en/latest/index.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HDUqB0IHrRa"
      },
      "source": [
        "# install monai framework\n",
        "!pip install tqdm --upgrade --quiet\n",
        "!pip install monai --quiet\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.disable(logging.WARNING)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rnGAJIC3WYk"
      },
      "source": [
        "from monai.transforms import Compose, LoadNiftiD, AddChannelD, ScaleIntensityD, ToTensorD\n",
        "from monai.apps import DecathlonDataset\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "        LoadNiftiD(keys=[\"image\", \"label\"]),\n",
        "        AddChannelD(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityD(keys=\"image\"),\n",
        "        ToTensorD(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "train_data = DecathlonDataset(\n",
        "    root_dir=\"./\", task=\"Task04_Hippocampus\", transform=transform, section=\"training\", seed=12345, download=True\n",
        ")\n",
        "val_data = DecathlonDataset(\n",
        "    root_dir=\"./\", task=\"Task04_Hippocampus\", transform=transform, section=\"validation\", seed=12345, download=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ7V7xu7Cz8A"
      },
      "source": [
        "Check the dataset size, shape and value ranges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNYJNsDk31Z7"
      },
      "source": [
        "import random\n",
        "\n",
        "idx = random.randint(0, len(train_data))\n",
        "image = train_data[idx]['image']\n",
        "label = train_data[idx]['label']\n",
        "print(f'train dataset size: {len(train_data)}')\n",
        "print(f'validation dataset size: {len(val_data)}')\n",
        "print(f'\\nrandom index: {idx}')\n",
        "print(f'\\nimage shape', image.shape)\n",
        "print(f'label shape', label.shape)\n",
        "print(f'\\nimage range', image.min(), image.max())\n",
        "print(f'label range', label.min(), label.max())\n",
        "print(f'label unique values', label.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkiI4JGSC8OS"
      },
      "source": [
        "Plot random images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quxurlj64tx1"
      },
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "to_pillow = ToPILImage()\n",
        "\n",
        "idx = random.randint(0, len(train_data))\n",
        "# plot some input images\n",
        "sample = train_data[idx]\n",
        "\n",
        "input = sample['image'].squeeze()\n",
        "input_imgs = [input.select(i, input.shape[i]//3) for i in range(3)]\n",
        "\n",
        "label = sample['label'].squeeze()\n",
        "label_imgs = [label.select(i, label.shape[i]//3) for i in range(3)]\n",
        "\n",
        "fig,ax = plt.subplots(2,3, figsize=(12, 9))\n",
        "fig.suptitle(f'Hippocampus Dataset Sample - index {idx}', fontsize=16)\n",
        "for i in range(3):\n",
        "  ax[0,i].imshow(to_pillow(input_imgs[i]), cmap='gray')\n",
        "  if not i:\n",
        "    ax[0,i].set_ylabel('input', rotation=90, fontsize=20)\n",
        "for i in range(3):\n",
        "  ax[1,i].imshow(to_pillow(label_imgs[i]), cmap='gnuplot')\n",
        "  if not i:\n",
        "    ax[1,i].set_ylabel('label', rotation=90, fontsize=20)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5XHcRa57xmh"
      },
      "source": [
        "## U-Net Model\n",
        "\n",
        "We will be using a 3D version of the famous U-Net first published here by Olaf Ronneberger et al. from the University of Freiburg (https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).\n",
        "\n",
        "Here's the original paper: https://arxiv.org/abs/1505.04597\n",
        "\n",
        "![U-Net Architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
        "\n",
        "This is a very basic implementation of the original 2D U-Net with minor improvements (e.g. optional batch normalization):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wArIWQqNhkUf"
      },
      "source": [
        "# adapted from: https://github.com/jvanvugt/pytorch-unet/blob/master/unet.py\n",
        "# adapted from https://discuss.pytorch.org/t/unet-implementation/426\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from math import log\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=1,\n",
        "        n_classes=2,\n",
        "        num_filters=64,\n",
        "        depth=5,\n",
        "        padding=False,\n",
        "        batch_norm=False,\n",
        "        up_mode='upconv',\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        (Ronneberger et al., 2015)\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "        Using the default arguments will yield the exact version used\n",
        "        in the original paper\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            n_classes (int): number of output channels\n",
        "            depth (int): depth of the network\n",
        "            wf (int): number of filters in the first layer is 2**wf\n",
        "            padding (bool): if True, apply padding such that the input shape\n",
        "                            is the same as the output.\n",
        "                            This may introduce artifacts\n",
        "            batch_norm (bool): Use BatchNorm after layers with an\n",
        "                               activation function\n",
        "            up_mode (str): one of 'upconv' or 'upsample'.\n",
        "                           'upconv' will use transposed convolutions for\n",
        "                           learned upsampling.\n",
        "                           'upsample' will use bilinear upsampling.\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        assert up_mode in ('upconv', 'upsample')\n",
        "        wf=int(log(num_filters, 2))\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(\n",
        "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path.append(\n",
        "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks = []\n",
        "        # encoder\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            if i != len(self.down_path) - 1:\n",
        "                blocks.append(x)\n",
        "                x = F.max_pool2d(x, 2)\n",
        "        # decoder\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "\n",
        "        return self.last(x)\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        block = []\n",
        "\n",
        "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        if up_mode == 'upconv':\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
        "        elif up_mode == 'upsample':\n",
        "            self.up = nn.Sequential(\n",
        "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
        "            )\n",
        "\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
        "\n",
        "    def center_crop(self, layer, target_size):\n",
        "        _, _, layer_height, layer_width = layer.size()\n",
        "        diff_y = (layer_height - target_size[0]) // 2\n",
        "        diff_x = (layer_width - target_size[1]) // 2\n",
        "        return layer[\n",
        "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
        "        ]\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
        "        out = torch.cat([up, crop1], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lPDtAHlFlnY"
      },
      "source": [
        "Today we will not be using this one but an improved yet still very basic 3D version with residual connnections but the core architecture is still the same as the original U-Net. More details here https://docs.monai.io/en/latest/networks.html#basicunet and here https://www.nature.com/articles/s41592-018-0261-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KboZrNXjRYGh"
      },
      "source": [
        "## Segmentation loss function: DICE Loss\n",
        "\n",
        "Before we can setup our training pipeline we need a a loss function. For a semantic segmentation task, the go-to loss for the beginning should be the DICE losss. The DICE loss is one of the most popular contributions of our chair and Prof. Dr. Nassir Navab to the Deep Learning community. It was first published by Fausto Milletari et. al. in 2016 in their publication [*V-Net: Fully Convolutional Neural Networks forVolumetric Medical Image Segmentation*](https://arxiv.org/abs/1606.04797) and is based on the [Sørensen–Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient). The DICE score is equivalent to the F1 score in a classification task.\n",
        "\n",
        "![DICE Formula](https://wikimedia.org/api/rest_v1/media/math/render/svg/a80a97215e1afc0b222e604af1b2099dc9363d3b)\n",
        "\n",
        "The core idea is to directly optimize for the DICE coefficient in a semantic image segmentation task by making it differentiable. It is calculated by dividing the doubled sum of the intersection of the predictions and groundtruth by the union of predictions and groundtruth.\n",
        "\n",
        "\n",
        "A good visualization can be found here:\n",
        "\n",
        "![DICE loss](https://miro.medium.com/max/486/1*yUd5ckecHjWZf6hGrdlwzA.png)\n",
        "\n",
        "https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHPQ49xbRdY2"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch import einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DiceLoss():\n",
        "  '''\n",
        "  Calculates the DICE loss per channel\n",
        "  '''\n",
        "  def __call__(self, segmentation, labelmap):\n",
        "    # convert label encoding e.g. 0,1,2 to one-hot-encoded vectors\n",
        "    # we do this to bring the labelmaps from the dataset to the same format as the networks output (which is one-hot-encoded)\n",
        "    labelmap = F.one_hot(labelmap.long(), num_classes=segmentation.shape[1]).squeeze(dim=0).movedim(-1, 1).float()\n",
        "    # you could just use the monai DICE loss: https://docs.monai.io/en/latest/losses.html#monai.losses.DiceLoss\n",
        "    smooth = 1e-5 # to avoid division by zero\n",
        "    # summing up all voxels where groundtruth has been predicted correctly over all spatial dimensions and the batch\n",
        "    #  -> vector of size of c\n",
        "    intersection = einsum(\"bcxyz, bcxyz->c\", segmentation, labelmap) + smooth\n",
        "    # summing up all voxels predicted for class c -> vector of size of c\n",
        "    prediction = einsum(\"bcxyz, bcxyz->c\", segmentation, segmentation)\n",
        "    # summing up all ground truth voxels for class c -> vector of size of c\n",
        "    groundtruth = einsum(\"bcxyz, bcxyz->c\", labelmap, labelmap)\n",
        "    # adding up\n",
        "    union = prediction + groundtruth + smooth\n",
        "    # DICE formula for the differentiable DICE loss per class c\n",
        "    DICE = 2.0 * intersection / union\n",
        "    # a loss needs to decrease in order to be optimized so we need to make it negative\n",
        "    # we choose 1 - dice_loss just to leave the losss positive -dice_loss would work just as well\n",
        "    dice_loss = 1 - DICE\n",
        "    # return the dice loss per channel (vector with the size of classes)\n",
        "    return dice_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8DKF8As9kzh"
      },
      "source": [
        "## Pytorch Lightning Module\n",
        "\n",
        "This is our favorite framework for setting up pytorch projects: https://pytorchlightning.ai/\n",
        "\n",
        "Check out the documentation: https://pytorch-lightning.readthedocs.io/en/stable/\n",
        "\n",
        "![Pytorch Lightning Idea](https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/docs/source/_images/general/pl_quick_start_full_compressed.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXNowmA9K4dB"
      },
      "source": [
        "### Lightning module\n",
        "extension of pytorch models with a simple training interface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6DviXzVcoTm"
      },
      "source": [
        "# first we need to install pytorch lightning\n",
        "! pip install pytorch-lightning --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq5kWyMs-ozZ"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from monai.networks.nets import BasicUNet\n",
        "import torch\n",
        "\n",
        "class SegmentationModule(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, in_channels=1, n_classes=3):\n",
        "    super().__init__()\n",
        "\n",
        "    # load unet as defined above as the backbone of our segmentation model\n",
        "    # see https://docs.monai.io/en/latest/networks.html#basicunet \n",
        "    # and here https://www.nature.com/articles/s41592-018-0261-2.\n",
        "    self.unet = BasicUNet(\n",
        "      dimensions=3, # using 3d images\n",
        "      in_channels=in_channels, # we just have one grey channel as input\n",
        "      out_channels=n_classes, # binary segmentation -> just one output channel\n",
        "      features=(32, 32, 64, 128, 256, 32),\n",
        "    )\n",
        "\n",
        "    # final layer for activation i.e. converting the logits to a value between 0 and 1\n",
        "    self.activation = nn.Softmax(dim=1)\n",
        "    \n",
        "    # you could just use the monai DICE loss: https://docs.monai.io/en/latest/losses.html#monai.losses.DiceLoss\n",
        "    self.dice_loss = DiceLoss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = {}\n",
        "    output['logits'] = self.unet(x)\n",
        "    output['probabilities'] = self.activation(output['logits'])\n",
        "    output['predictions'] = output['probabilities'].gt(0.5).float() # set threshold for positive prediction to > 0.5\n",
        "    return output\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    segmentation = self(batch['image'])\n",
        "    labelmap = batch['label']\n",
        "    # for the dice loss we take the mean of all channels including the background\n",
        "    loss = self.dice_loss(segmentation['probabilities'], labelmap).mean()\n",
        "    # in the metric we are only interested in the two target classes\n",
        "    dice_metric = 1 - self.dice_loss(segmentation['predictions'], labelmap)[1:].mean()\n",
        "    self.log('train_DICE', dice_metric, on_step=False, on_epoch=True, prog_bar=True)\n",
        "    self.log('loss', loss, on_step=False, on_epoch=True, prog_bar=False)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    segmentation = self(batch['image'])\n",
        "    labelmap = batch['label']\n",
        "    loss = self.dice_loss(segmentation['probabilities'], labelmap).mean()\n",
        "    dice_metric = 1 - self.dice_loss(segmentation['predictions'], labelmap)[1:].mean()\n",
        "    self.log('val_loss', loss, prog_bar=True)\n",
        "    self.log('val_DICE', dice_metric, prog_bar=True)\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters(), lr=5e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnIM0WHHLAhn"
      },
      "source": [
        "### Data module \n",
        "just a fancy dataloader with the same dataset we loaded above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tyoqk1eC6qp"
      },
      "source": [
        "from monai.apps import DecathlonDataset\n",
        "from monai.transforms import Compose, LoadNiftiD, AddChannelD, ScaleIntensityD, ToTensorD\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class HippocampusData(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, data_dir=\"./\", batch_size=1):\n",
        "    super().__init__()\n",
        "    self.data_dir = data_dir\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    # very basic transformations check out: https://docs.monai.io/en/latest/transforms.html\n",
        "    self.transform = Compose(\n",
        "      [\n",
        "        LoadNiftiD(keys=[\"image\", \"label\"]),\n",
        "        AddChannelD(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityD(keys=\"image\"),\n",
        "        ToTensorD(keys=[\"image\", \"label\"]),\n",
        "      ]\n",
        "    )\n",
        "    self.train_data = DecathlonDataset(\n",
        "      root_dir=self.data_dir, \n",
        "      task=\"Task04_Hippocampus\", \n",
        "      transform=transform, \n",
        "      section=\"training\", \n",
        "      seed=42, \n",
        "      download=True\n",
        "    )\n",
        "    self.val_data = DecathlonDataset(\n",
        "      root_dir=self.data_dir, \n",
        "      task=\"Task04_Hippocampus\", \n",
        "      transform=transform, \n",
        "      section=\"validation\", \n",
        "      seed=42, \n",
        "      download=True\n",
        "    )\n",
        "\n",
        "  def train_dataloader(self):\n",
        "      return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "      return DataLoader(self.val_data, batch_size=self.batch_size, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-llbETtAPUh9"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohF1VK6YZSv4"
      },
      "source": [
        "Training setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTgtsECWOzc_"
      },
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# setting up our pytorch lightning modules\n",
        "model = SegmentationModule()\n",
        "data = HippocampusData()\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"hippo 3D unet\")\n",
        "\n",
        "# setting up our pytorch lightning trainer\n",
        "trainer = pl.Trainer(\n",
        "  gpus=1, \n",
        "  max_epochs=10, \n",
        "  progress_bar_refresh_rate=20,\n",
        "  limit_train_batches=50, # we just look at 50 random samples per epoch to shorten them\n",
        "  logger=logger\n",
        "  # weights_summary='full', # if you are curious about the model uncomment this\n",
        ")\n",
        "print('training setup finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXz6-kZ5ZVqI"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMUW9cTFXiQ3"
      },
      "source": [
        "trainer.fit(model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aK7FZxFZY0N"
      },
      "source": [
        "Plot some predictions with input and groundtruth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-biMH1ckF42"
      },
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "to_pillow = ToPILImage()\n",
        "\n",
        "idx = random.randint(0, len(train_data))\n",
        "# plot some input images\n",
        "sample = train_data[idx]\n",
        "\n",
        "input = sample['image'].squeeze()\n",
        "input_imgs = [input.select(i, input.shape[i]//3) for i in range(3)]\n",
        "\n",
        "label = sample['label'].squeeze()\n",
        "label_imgs = [label.select(i, label.shape[i]//3) for i in range(3)]\n",
        "\n",
        "prediction = model(sample['image'].unsqueeze(dim=0))['predictions']\n",
        "\n",
        "dice_score = 1 - model.dice_loss(prediction, sample['label'].unsqueeze(dim=0))[1:].mean()\n",
        "\n",
        "prediction = prediction.squeeze().argmax(dim=0).float()\n",
        "\n",
        "pred_imgs = [prediction.select(i, prediction.shape[i]//3) for i in range(3)]\n",
        "\n",
        "fig,ax = plt.subplots(3,3, figsize=(12, 9))\n",
        "fig.suptitle(f'Hippocampus 3D-Unet Segmentation - index {idx} - DICE {dice_score:.2%}', fontsize=16)\n",
        "for i in range(3):\n",
        "  ax[0,i].imshow(to_pillow(input_imgs[i]), cmap='gray')\n",
        "  if not i:\n",
        "    ax[0,i].set_ylabel('input', rotation=90, fontsize=20)\n",
        "for i in range(3):\n",
        "  ax[1,i].imshow(to_pillow(label_imgs[i]), cmap='gnuplot')\n",
        "  if not i:\n",
        "    ax[1,i].set_ylabel('label', rotation=90, fontsize=20)\n",
        "for i in range(3):\n",
        "  ax[2,i].imshow(to_pillow(pred_imgs[i]), cmap='gnuplot')\n",
        "  if not i:\n",
        "    ax[2,i].set_ylabel('prediction', rotation=90, fontsize=20)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so_hihVnb4aS"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgjJZHNgVGLG"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iYHVjqDhTMx"
      },
      "source": [
        "## What next?\n",
        "\n",
        "Dive into the documentation of Pytorch Lightning and MONAI:\n",
        "\n",
        "https://pytorch-lightning.readthedocs.io/en/stable/\n",
        "https://docs.monai.io/en/latest/index.html\n",
        "\n",
        "*   Optimize the training and do some hyper parameter tuning!\n",
        "*   Try your own models\n",
        "*   Try other datasets\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBelb6etgwDH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}