{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Exercise_8_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5yOkGTUF6Kp"
      },
      "source": [
        "# MNIST (Modified National Institute of Standards and Technology)\n",
        "\n",
        "This week we do everything on MNIST database due to time constraints: \n",
        "https://en.wikipedia.org/wiki/MNIST_database\n",
        "\n",
        "![MNIST](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuv6dlX5VEdf"
      },
      "source": [
        "# Imports\n",
        "!pip install --upgrade -q gspread\n",
        "!pip install 'google-auth==1.16.1'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import datetime\n",
        "import pytz\n",
        "\n",
        "tz = pytz.timezone('Europe/Berlin')\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtR_EloyVUT_"
      },
      "source": [
        "The first time you run this there is probably an error. Just click on \"RUNTIME\" --> \"RESTART RUNTIME\" and run this cell again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YV1_mNAVpOd"
      },
      "source": [
        "student_name = \"yourName\"\n",
        "assert student_name != \"yourName\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYKNls6aV3Zm"
      },
      "source": [
        "# init google sheet methods for task submitting\n",
        "gsheet = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1MZcZXXADcBUHjrdFIRNSAhE0oChqx4sRJ2odUCJ6qJ4/edit?usp=sharing\")\n",
        "\n",
        "def write_result(task_number, result=None):\n",
        "  if task_number == 'homework':\n",
        "    worksheet = gsheet.worksheet(\"homework\".format(task_number))\n",
        "  else:\n",
        "    worksheet = gsheet.worksheet(\"task{}\".format(task_number))\n",
        "  current_time = datetime.datetime.now(tz).strftime(\"%X\")\n",
        "  current_date = str(datetime.date.today())\n",
        "  if result:\n",
        "    worksheet.append_row([student_name, current_time, current_date, result])\n",
        "    print(\"Task {} successfully solved by {} at {} with result: {}\".format(task_number, student_name, current_time, result))\n",
        "  else:\n",
        "    worksheet.append_row([student_name, current_time, current_date])\n",
        "    print(\"Task {} successfully solved by {} at {}\".format(task_number, student_name, current_time))\n",
        "\n",
        "print(\"Reporting enabled - write_result(number_of_task, result='your result') \")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLsnKskobGob"
      },
      "source": [
        "# quick check everything is working for you\n",
        "\n",
        "my_favorite_animal =\n",
        "assert my_favorite_animal, 'What is your favorite animal???!'\n",
        "\n",
        "write_result(0, my_favorite_animal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtwesOsZF6K6"
      },
      "source": [
        "# names for our labels = just the digits\n",
        "classes=[str(i) for i in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjF-Aq9BF6K_"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "# define the transformations the images go through each time it is used for training\n",
        "# includes augmentation AND normalization as described above\n",
        "augmentation_train = transforms.Compose([\n",
        "                                  # randomly perform a horizontal flip of the image\n",
        "                                  transforms.RandomHorizontalFlip(),\n",
        "                                  # rotate the image with a angle from 0 to 60 (chosen randomly)\n",
        "                                  transforms.RandomRotation(degrees=60),\n",
        "                                  # convert the image into a tensor so it can be processed by the GPU\n",
        "                                  transforms.ToTensor(),\n",
        "                                  # normalize the image with the mean and std of ImageNet\n",
        "                                   ])\n",
        "\n",
        "\n",
        "# no augmentation for the test data only resizing, conversion to tensor and normalization\n",
        "augmentation_test = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEj7Jl_nHzwN"
      },
      "source": [
        "# Task 1: load the MNIST dataset that is provided by pytorch \n",
        "Here's an overview of datasets\n",
        "https://pytorch.org/docs/stable/torchvision/datasets.html\n",
        "\n",
        "Hints:\n",
        "- datasets can be found in torchvision.dataset.\n",
        "- root can just be your base folder e.g. \".\"\n",
        "- pick the test part for validation\n",
        "- don't forget to add the augmentations from above\n",
        "- make sure to enable the download\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v194TjXLF6LH"
      },
      "source": [
        "import torchvision\n",
        "toy_dataset_train =\n",
        "toy_dataset_val ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKLdUbv6t0Qs"
      },
      "source": [
        "text = f\"\\n\\nSize of training dataset: {len(toy_dataset_train)} \\nSize of validation set: {len(toy_dataset_val)}\"\n",
        "assert (len(toy_dataset_train)+len(toy_dataset_val))==70000, 'Hmm? Is the MNIST dataset loaded?'\n",
        "write_result(1, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX3XXG9ed24J"
      },
      "source": [
        "# import of a couple more modules we need later for progress bars and plots\n",
        "from tqdm import tnrange, tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foGNkuwzF6LV"
      },
      "source": [
        "# Declare the network\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Running on device:\",device)\n",
        "num_classes=10\n",
        "\n",
        "class OurNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OurNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, (3,3)) \n",
        "        self.fc1   = nn.Linear(16*13*13, 120)\n",
        "        self.fc2   = nn.Linear(120, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvSBNhkBgD6D"
      },
      "source": [
        "# make sure we are running on GPU\n",
        "assert str(device) == \"cuda\", 'If this fails maybe change the runtime: Runtime -> Change runtime type'\n",
        "print('Nice! Running on the GPU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWnvZpk2l7_j"
      },
      "source": [
        "# declare network and send it to device=cuda\n",
        "net = OurNet()\n",
        "net = net.to(device)\n",
        "summary(net, input_size=(1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCJn-DV5F6Lc"
      },
      "source": [
        "# Median Frequency Balancing, last wekk ...\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# get the class labels of each image\n",
        "class_labels = toy_dataset_train.targets\n",
        "# empty array for counting instance of each class\n",
        "count_labels = np.zeros(len(classes))\n",
        "# empty array for weights of each class\n",
        "class_weights = np.zeros(len(classes))\n",
        "\n",
        "# populate the count array\n",
        "for l in class_labels:\n",
        "  count_labels[l] += 1\n",
        "\n",
        "# get median count\n",
        "median_freq = np.median(count_labels)\n",
        "#print(median_freq)\n",
        "#print(count_labels)\n",
        "\n",
        "# calculate the weigths\n",
        "for i in range(len(classes)):\n",
        "  class_weights[i] = median_freq/count_labels[i]\n",
        "\n",
        "# print the weights\n",
        "for i in range(len(classes)):\n",
        "    print(classes[i],\":\", class_weights[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pPUGMq3NCnP"
      },
      "source": [
        "Now we define the loss function with the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-Q72RJ-M84P"
      },
      "source": [
        "# put the weights on our device\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAAQG2sWVrvW"
      },
      "source": [
        "# Task 2: define the loss for multicalss classificaiton\n",
        "\n",
        "Overview of losses available: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\n",
        "Hints:\n",
        "- CrossEntropy-Loss is a good choice\n",
        "- use the nn module we have already imported as nn.\n",
        "- dont forget to add the weights we just put on our device for class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYEg6DByhIxi"
      },
      "source": [
        "# loss function\r\n",
        "criterion ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFNz9lBFg06p"
      },
      "source": [
        "# submitt your results\n",
        "assert criterion.weight is not None, 'Did you specify the weight argument?'\n",
        "write_result(2, result=str(criterion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcGGs0Bk8eCU"
      },
      "source": [
        "# Evaluation functions\n",
        "\n",
        "Here we write a function which calculates the accuracy of model based on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tVHzZn78eCW"
      },
      "source": [
        "# functions for evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def get_num_correct(predicted, labels):\n",
        "    '''\n",
        "    This function is used to decide if the predicted and ground truth classes are same or not.\n",
        "    args: \n",
        "      predicted = output of model\n",
        "      labels = true output\n",
        "    '''\n",
        "    batch_len, correct= 0, 0\n",
        "    batch_len = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    # correct = [1,1,1,1,2,3,4,5] predicted = [1,2,1,1,,2,3,4,6]\n",
        "    return batch_len, correct\n",
        "\n",
        "# function for validation step\n",
        "def evaluate(model, val_loader):\n",
        "    '''\n",
        "    This function calculates the total accuracy of the model on the validation set.\n",
        "    args:\n",
        "      model = the network we want to evaluate\n",
        "      val_loader = the validation data loader\n",
        "    '''\n",
        "    losses= 0\n",
        "    num_samples_total=0\n",
        "    correct_total=0\n",
        "    model.eval()\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        out = model(inputs)\n",
        "        _, predicted = torch.max(out, 1)\n",
        "        loss = criterion(out, labels)\n",
        "        losses += loss.item() \n",
        "        b_len, corr = get_num_correct(predicted, labels)\n",
        "        num_samples_total +=b_len\n",
        "        correct_total +=corr\n",
        "    accuracy = correct_total/num_samples_total\n",
        "    losses = losses/len(val_loader)\n",
        "    return losses, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oGr-AfqF6Ln"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyc3or5oF6Lo"
      },
      "source": [
        "# define the batch size\n",
        "batch_size, validation_batch_size = 4096, 4096\n",
        "\n",
        "import datetime\n",
        "start_time = datetime.datetime.now()\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
        "\n",
        "num_w = 0\n",
        "# create and instance of a dataloader for training\n",
        "train_data_loader = torch.utils.data.DataLoader(toy_dataset_train, batch_size=batch_size, shuffle=True,num_workers=num_w)\n",
        "validation_data_loader = torch.utils.data.DataLoader(toy_dataset_val, batch_size=validation_batch_size, shuffle=True,  num_workers=num_w)\n",
        "# number of loops over the dataset\n",
        "\n",
        "num_epochs = 5\n",
        "accuracy = []\n",
        "val_accuracy = []\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Started Training\")\n",
        "epoch = 0\n",
        "running_loss = 0.0\n",
        "correct_total= 0.0\n",
        "num_samples_total=0.0\n",
        "train_accuracy = 0.0\n",
        "val_acc=0.0\n",
        "val_loss = 0.0\n",
        "print('Epoch: %d' %(epoch+1))\n",
        "print('Train Loss: %.3f  Train Accuracy:%.3f' %(running_loss, train_accuracy))\n",
        "print('Validation Loss: %.3f  Val Accuracy: %.3f' %(val_loss, val_acc))\n",
        "\n",
        "for epoch in tnrange(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_total= 0.0\n",
        "    num_samples_total=0.0\n",
        "    \n",
        "    print(\"...\")\n",
        "    for i, data in tqdm_notebook(enumerate(train_data_loader)):\n",
        "        if i % 2 == 0:\n",
        "            print(f\"Iteration: {i+1}/{len(train_data_loader)}\")\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # set the parameter gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #compute accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        b_len, corr = get_num_correct(predicted, labels)\n",
        "        num_samples_total +=b_len\n",
        "        correct_total +=corr\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    \n",
        "    running_loss /= len(train_data_loader)\n",
        "    train_accuracy = correct_total/num_samples_total\n",
        "    val_loss, val_acc = evaluate(net, validation_data_loader)\n",
        "    \n",
        "    print('Epoch: %d' %(epoch+1))\n",
        "    print('Train Loss: %.3f  Train Accuracy:%.3f' %(running_loss, train_accuracy))\n",
        "    print('Validation Loss: %.3f  Val Accuracy: %.3f' %(val_loss, val_acc))\n",
        "\n",
        "    losses.append(running_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    accuracy.append(train_accuracy)\n",
        "    val_accuracy.append(val_acc)\n",
        "print('Finished Training')\n",
        "end_time = datetime.datetime.now()\n",
        "delta = end_time -start_time\n",
        "print(\"Time for training of {} Epochs is {}s\".format(num_epochs,delta.seconds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aTte4fcYMT0"
      },
      "source": [
        "# Task 3: Why are the validation metrics better than the training metrics???\n",
        "Didn't we learn, that this should be the other way around?!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxkSsG11m6y_"
      },
      "source": [
        "# submitt your results\n",
        "my_answer = \n",
        "assert my_answer is not None, 'Give it a try'\n",
        "write_result(3, result=my_answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw_TaHF98eCf"
      },
      "source": [
        "# Train and Validation Curves\n",
        "\n",
        "To better understand whether our network is actually learning something, we plot the training and validation curves.\n",
        "\n",
        "There are two types of curves:\n",
        "- Loss Curves: Plotting the trend of the loss per epoch.\n",
        "- Accuracy Curves: Plotting accuracy, that is the performance of our model per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwjnpkM5HDBO"
      },
      "source": [
        "# plot curves\n",
        "from tqdm import tnrange, tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "num_epochs=5\n",
        "print(num_epochs)\n",
        "epoch = range(1, num_epochs+1)\n",
        "fig = plt.figure(figsize=(10, 15))\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epoch, losses, label='Training loss')\n",
        "plt.plot(epoch, val_losses, label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "#plt.figure()\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 15))\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epoch, accuracy, label='Training accuracy')\n",
        "plt.plot(epoch, val_accuracy, label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "#plt.figure()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_inKdz_9xzi"
      },
      "source": [
        "# Common Training Errors\n",
        "\n",
        "Let us look at some errors and how to detect them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQoSUM41_IIC"
      },
      "source": [
        "## Data Scarcity\n",
        "\n",
        "We need enough data to learn a good model. Less data means less learning.\n",
        "\n",
        "Let's see how the performance is affected when we have less data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6P-z61Bl8A7"
      },
      "source": [
        "# as before, just bigger network\n",
        "from torchsummary import summary\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Running on:\",device)\n",
        "class OurNetBig(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OurNetBig, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, (3,3), padding=2)\n",
        "        self.conv3 = nn.Conv2d(16, 32, (3,3)) \n",
        "        self.fc1   = nn.Linear(32*13*13, 1000)\n",
        "        self.fc2   = nn.Linear(1000, 120)\n",
        "        self.fc3   = nn.Linear(120, 10)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "net_data = OurNetBig()\n",
        "net_data = net_data.to(device)\n",
        "summary(net_data, input_size=(1,28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1HSj_HeC0nn"
      },
      "source": [
        "# choose subset to imitate scarce data\n",
        "from torch.utils.data import Subset\n",
        "batch_size = 50\n",
        "subset_size = 200\n",
        "\n",
        "length_dataset = [i for i in range(subset_size)]\n",
        "small_toy_dataset_train = Subset(toy_dataset_train,length_dataset)\n",
        "train_data_loader_small = torch.utils.data.DataLoader(small_toy_dataset_train, batch_size=batch_size, shuffle=True,num_workers=4)\n",
        "validation_data_loader = torch.utils.data.DataLoader(toy_dataset_val, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
        "\n",
        "print(f\"small dataset size: {len(small_toy_dataset_train)} batch_size: {batch_size} number of batches in dataloader: {len(train_data_loader_small)} \")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l1KoQT-qOy5"
      },
      "source": [
        "# weight initialisation\n",
        "def weight_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "net_data.apply(weight_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3J_A9Iy_O1c"
      },
      "source": [
        "# as before...\n",
        "# optimizer\n",
        "import torch.optim as optim \n",
        "\n",
        "\n",
        "num_epochs = 25\n",
        "accuracy = []\n",
        "val_accuracy = []\n",
        "losses = []\n",
        "val_losses = []\n",
        "running_loss= 0.0\n",
        "train_accuracy = 0.0\n",
        "val_loss = 0.0\n",
        "val_acc = 0.0\n",
        "epoch=0\n",
        "\n",
        "print('Epoch: %d' %(epoch+1))\n",
        "print('Loss: %.3f  Accuracy:%.3f' %(running_loss, train_accuracy))\n",
        "print('Validation Loss: %.3f  Val Accuracy: %.3f' %(val_loss, val_acc))\n",
        "\n",
        "# intialize the network\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net_data.parameters(), lr=1e-2)\n",
        "print(net_data)\n",
        "print(\"Started Training\")\n",
        "\n",
        "for epoch in tnrange(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_total= 0.0\n",
        "    num_samples_total=0.0\n",
        "    for i, data in tqdm_notebook(enumerate(train_data_loader_small)):\n",
        "        #print(f\"Iteration: {i+1}/{len(train_data_loader_small)}\")\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # set the parameter gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net_data(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #compute accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        b_len, corr = get_num_correct(predicted, labels)\n",
        "        num_samples_total +=b_len\n",
        "        correct_total +=corr\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    \n",
        "    running_loss /= len(train_data_loader_small)\n",
        "    train_accuracy = correct_total/num_samples_total\n",
        "    val_loss, val_acc = evaluate(net_data, validation_data_loader)\n",
        "    \n",
        "    print('Epoch: %d' %(epoch+1))\n",
        "    print('Loss: %.3f  Accuracy:%.3f' %(running_loss, train_accuracy))\n",
        "    print('Validation Loss: %.3f  Val Accuracy: %.3f' %(val_loss, val_acc))\n",
        "\n",
        "    losses.append(running_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    accuracy.append(train_accuracy)\n",
        "    val_accuracy.append(val_acc)\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqUlwE_C_O1o"
      },
      "source": [
        "# lets look at the plots\n",
        "\n",
        "epoch = range(1, num_epochs+1)\n",
        "fig = plt.figure(figsize=(10, 15))\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epoch, losses, label='Training loss')\n",
        "plt.plot(epoch, val_losses, label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 15))\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epoch, accuracy, label='Training accuracy')\n",
        "plt.plot(epoch, val_accuracy, label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv0itQCNDFee"
      },
      "source": [
        "## Learning Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWm2mVDpE1C3"
      },
      "source": [
        "# new network to test new learning rate\n",
        "net_lr = OurNet()\n",
        "net_lr = net_lr.to(device)\n",
        "\n",
        "# optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer_lr = optim.Adam(net_lr.parameters(), lr=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpTjIJfEDFek"
      },
      "source": [
        "# number of loops over the dataset\n",
        "num_epochs = 5\n",
        "accuracy = []\n",
        "val_accuracy = []\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in tnrange(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_total= 0.0\n",
        "    num_samples_total=0.0\n",
        "    for i, data in tqdm_notebook(enumerate(train_data_loader)):\n",
        "        print(f\"Iteration: {i+1}/{len(train_data_loader)}\")\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # set the parameter gradients to zero\n",
        "        optimizer_lr.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net_lr(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_lr.step()\n",
        "        \n",
        "        #compute accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        b_len, corr = get_num_correct(predicted, labels)\n",
        "        num_samples_total +=b_len\n",
        "        correct_total +=corr\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    \n",
        "    running_loss /= len(train_data_loader)\n",
        "    train_accuracy = correct_total/num_samples_total\n",
        "    val_loss, val_acc = evaluate(net_lr, validation_data_loader)\n",
        "    \n",
        "    print('Epoch: %d' %(epoch+1))\n",
        "    print('Loss: %.3f  Accuracy:%.3f' %(running_loss, train_accuracy))\n",
        "    print('Validation Loss: %.3f  Val Accuracy: %.3f' %(val_loss, val_acc))\n",
        "\n",
        "    losses.append(running_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    accuracy.append(train_accuracy)\n",
        "    val_accuracy.append(val_acc)\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nimt5PfzDFex"
      },
      "source": [
        "# plot the graphs\n",
        "\n",
        "epoch = range(1, num_epochs+1)\n",
        "fig = plt.figure(figsize=(10, 15))\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epoch, losses, label='Training loss')\n",
        "plt.plot(epoch, val_losses, label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 15))\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epoch, accuracy, label='Training accuracy')\n",
        "plt.plot(epoch, val_accuracy, label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri_Xkb97FKh1"
      },
      "source": [
        "# Homework - Train a model which achieves at least 80% accuracy on HAM10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnbscZH78eCv"
      },
      "source": [
        "# your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72AUzTgnkrB3"
      },
      "source": [
        "# share your results with us when you are finished\n",
        "describe_your_solution = ''\n",
        "model_accuracy = 0\n",
        "assert model_accuracy, \"fill in your best accuracy\"\n",
        "write_result('homework', f\"{describe_your_solution}\\n\\n{model_accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}