{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Exercise_6_solution.ipynb","provenance":[{"file_id":"1wYWZ0q4oBQmMYb6yR046fVHAwZUv8xMh","timestamp":1606156637176},{"file_id":"https://github.com/IFL-CAMP/ML4MS_2020/blob/master/exercises/Exercise_5.ipynb","timestamp":1591781871348}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6-final"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"c7j5FQ4IlSeV"},"source":["# Get the data (Recap)\n","\n","First we need access to data.\n","- You can use this link to add the data to your drive: https://drive.google.com/drive/folders/1pHNxZVrlcKh5usWoNC_V7gR2WdeDutjv\n","- If you have not done this yet, right click on the **CS4MS_Data** folder and click on the **Add shortcut to Drive** option.\n","- Inside the folder **CS4MS_Data** you will see the folder **HAM10000** - this is the dataset (set of images) we will be working with.\n","\n","Now you can run the next cell"]},{"cell_type":"code","metadata":{"id":"r0G9KUdljfrl"},"source":["# Imports\n","!pip install --upgrade -q gspread\n","from google.colab import auth\n","auth.authenticate_user()\n","import gspread\n","from oauth2client.client import GoogleCredentials\n","\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","import torchvision\n","import numpy as np\n","import random\n","\n","import datetime\n","import pytz\n","\n","tz = pytz.timezone('Europe/Berlin')\n","\n","gc = gspread.authorize(GoogleCredentials.get_application_default())\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRrBmXZOa3hX"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFZmhYBylSeZ"},"source":["data_dir = \"/content/drive/My Drive/CS4MS_Data/HAM10000\"\n","\n","classes = [ 'actinic keratoses', 'basal cell carcinoma', 'benign keratosis-like lesions', \n","           'dermatofibroma','melanoma', 'melanocytic nevi', 'vascular lesions']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ebg3uPH4jfrv"},"source":["### Change your Name start ###\n","student_name = \"Tobias\"\n","### Change your Name end ###\n","\n","### Check if name changed ###\n","assert student_name != \"yourName\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCD9XrpTjfsD"},"source":["#@title Result Form\n","gsheet = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1i1lLpburGOuTb5wGx88cuQi0ItA7JOjoNIRbBhEyaH0/edit#gid=1494605435\")\n","\n","def write_result(task_number, result=None):\n","  worksheet = gsheet.worksheet(f\"task{task_number}\")\n","  current_time = datetime.datetime.now(tz).strftime(\"%X\")\n","  current_date = str(datetime.date.today())\n","  if result:\n","    worksheet.append_row([student_name, current_time, current_date, result])\n","    print(f\"Task {task_number} successfully solved by {student_name} at {current_time} with result: {result}\")\n","  else:\n","    worksheet.append_row([student_name, current_time, current_date])\n","    print(f\"Task {task_number} successfully solved by {student_name} at {current_time}\")\n","\n","print(\"Reporting enabled - write_result(number_of_task, result='your result') \")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCSW-Zi5RIh1"},"source":["# quick example for object oriented programming: working with paths (folders and files)\n","from pathlib import Path\n","gdrive_connection_success = Path(data_dir) \n","\n","if gdrive_connection_success.is_dir():\n","    write_result(0)\n","else:\n","    print(\"your folder is not mounted correclty - contact the tutors!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfvxo1CNlSec"},"source":["# Data Augmentation\n","\n","It is a common fact that medical data is scarce. But to learn a very good model, the network needs a lot of data. So to tackle the problem we perform data augmentation.\n","\n","Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. \n","\n","Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n","\n","![Data Augmentation](https://cdn-images-1.medium.com/max/1000/1*C8hNiOqur4OJyEZmC7OnzQ.png)\n","[Source](https://cdn-images-1.medium.com/max/1000/1*C8hNiOqur4OJyEZmC7OnzQ.png) \n","\n"]},{"cell_type":"code","metadata":{"id":"kRB51WV5jfsR"},"source":["# Downloading cat img\n","!wget https://raw.githubusercontent.com/IFL-CAMP/AI4MDs_21/main/images/cat.jpg\n","from PIL import Image\n","# Opening Cat img\n","cat = Image.open(\"cat.jpg\")\n","\n","#Visualizing cat img\n","plt.axis('off')\n","plt.imshow(cat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0F4QNQzjfsY"},"source":["def imshow(img):\n","    npimg = img.numpy()\n","    fig, ax = plt.subplots(figsize=(30, 30))\n","    ax.axis('off')\n","    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0LmWKewkjfsc"},"source":["augmentation = transforms.Compose([\n","                                  # resize image to the network input size\n","                                  transforms.Resize((224,224)),\n","                                  transforms.RandomHorizontalFlip(),\n","                                  transforms.RandomRotation(degrees=60),\n","                                  transforms.RandomCrop(180),\n","                                  transforms.ToTensor(),\n","                                   ])\n","# Complete list: https://pytorch.org/docs/stable/torchvision/transforms.html\n","# Examples: \n","# torchvision.transforms.RandomErasing()\n","# torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0) --> transforms.RandomAffine(degrees=20, shear=[0,50]),\n","# torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0) --> transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n","# transforms.GaussianBlur(kernel_size, sigma=(0.1, 2.0))\n","# transforms.RandomCrop(180),"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qawa8dzejfsh"},"source":["images = []\n","for i in range(16):\n","    temp_im = augmentation(cat)\n","    images.append(temp_im)\n","    \n","# show images\n","imshow(torchvision.utils.make_grid(images))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCq6cUxSjfsj"},"source":["# Try out your own transformations \n","task1_done=False\n","own_aug =  transforms.Compose([\n","                              transforms.Resize((224,224)),\n","                              torchvision.transforms.RandomAffine(degrees=20, shear=20, fillcolor=(255,0,0)),\n","                              transforms.ToTensor(),\n","                              torchvision.transforms.ColorJitter(hue=0.4)\n","                              ])\n","\n","images = []\n","for i in range(16):\n","    temp_im = own_aug(cat)\n","    images.append(temp_im)  \n","# show images\n","imshow(torchvision.utils.make_grid(images))\n","\n","\n","if len(own_aug.transforms) <= 2:\n","  print(\"You have to apply more than 2 transformations\")\n","else:\n","  task1_done = True\n","  print(\"task1 done!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_eVnzUYjfsn"},"source":["# Done with Task1\n","\n","Once you are happy with your augmentation submit your results\n","\n"]},{"cell_type":"code","metadata":{"id":"qGvq6UfGjfsp"},"source":["effect = \"rotation, shear, colorjitter\"\n","\n","\n","if task1_done and effect != \"write what the effect of your transformations is - if you want you can give it a score between 0-10\":\n","  effect += \"\\n\"+str(own_aug)\n","  write_result(1, effect)\n","else:\n","  print(\"you didnt solve task1 yet.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMKB9X3Iqekc"},"source":["## Normalization\n","Data normalization is an important step which ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network. \n","\n","Data normalization is done by subtracting the dataset mean from each image and then dividing the result by the dataset standard deviation. The distribution of such data would resemble a Gaussian curve centered at zero. \n","\n","Since, skin lesion images are natural images, we use the normalization values (mean and standard deviation) from [Imagenet dataset.](http://www.image-net.org/)\n","*norm_mean = (0.4914, 0.4822, 0.4465)*\n","\n","*norm_std = (0.2023, 0.1994, 0.2010)*\n","\n","This denotes mean and standard deviation for each channel(RGB) of an image.\n","\n","\n","We perform following data augmentation:\n","- Resize the image.\n","- Flipping the image horizontally.\n","- Randomly rotating image.\n","- Normalizing the image."]},{"cell_type":"code","metadata":{"id":"Ces4rS8jlSed"},"source":["\n","# Imagenet values\n","norm_mean = (0.4914, 0.4822, 0.4465)\n","norm_std = (0.2023, 0.1994, 0.2010)\n","\n","# define the transformaitons the images go through each time it is used for training\n","# includes augmentation AND normalization as descirbed above\n","augmentation_train = transforms.Compose([\n","                                  # resize image to the network input size\n","                                  transforms.Resize((224,224)),\n","                                  # randomly perform a horizontal flip of the image\n","                                  transforms.RandomHorizontalFlip(),\n","                                  # rotate the image with a angle from 0 to 60 (chosen randomly)\n","                                  transforms.RandomRotation(degrees=60),\n","                                  # convert the image into a tensor so it can be processed by the GPU\n","                                  transforms.ToTensor(),\n","                                  # normalize the image with the mean and std of ImageNet\n","                                  transforms.Normalize(norm_mean, norm_std),\n","                                   ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fv86Skt1qt1V"},"source":["images = []\n","for i in range(16):\n","    temp_im = augmentation_train(cat)\n","    images.append(temp_im)  \n","# show images\n","imshow(torchvision.utils.make_grid(images))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yNLSsXlYlSem"},"source":["# Loading the data\n","\n","Use the **torchvision.datasets.ImageFolder** dataset class. This class requires the dataset to be arranged into folders of their respective class or labels. We already provide the dataset in suitable preprocessed format.\n","\n","Here we also apply the augmentation that we defined above.\n","\n","You can check here : https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder"]},{"cell_type":"code","metadata":{"id":"61k4h0yXlSen"},"source":["import torchvision\n","\n","# create an instance of the image folder class to load images by classes defined with the folders given\n","dataset = torchvision.datasets.ImageFolder(root= data_dir, transform= augmentation_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kop4udupouUg"},"source":["Let's try to use the __getitem__ method of the ImageFolder class."]},{"cell_type":"code","metadata":{"id":"6i1HoAColSeq"},"source":["# Check the dimension of the 1000th image and its corresponding label\n","\n","image, label = dataset[1000]\n","print(\"Image Shape: {} \\n Label: {} \\n Lesion Type: {}\".format(image.shape, label, classes[label]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSLBtnbLtXXA"},"source":["## Task2\n","\n","sample 16 different random images from the dataset\n","hint: check out random.sample(range(0,10), 5)\n","\n","save each image in the images list and print its index and label before adding it. Also add the label to the label_list.\n","hint: do it in a for loop\n","\n","finally look at what you just created"]},{"cell_type":"code","metadata":{"id":"51osdMBGrxh9"},"source":["randomlist = random.sample(range(0, len(dataset)), 16)\n","print(randomlist)\n","label_list = [] #This has to be filled in the for loop\n","images = [] #This has to be filled in the for loop\n","for i in randomlist:\n","    temp_im, label = dataset[i]\n","    print(f\"adding index: {i} with label {label}\")\n","    images.append(temp_im)\n","    label_list.append(label)\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Ib3QnQFvSQm"},"source":["## Submit Task2"]},{"cell_type":"code","metadata":{"id":"-GOoSejxvtf9"},"source":["result2=\"Write here what youd when you looked at the visualized images and labels\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2zDEVoQuv7wP"},"source":["Once you noted what you observed in \"result\" simply run the next cell."]},{"cell_type":"code","metadata":{"id":"zRlGzHlPvVPu"},"source":["if len(label_list) == 16 and len(images) == 16 and result2!=\"Write here what you noticed when you looked at the visualized images and labels\":\n","  result2 += \"\\n\" + \"labels: \" + str(label_list) + \"\\n\" + \"indices\" + str(randomlist)\n","  write_result(2, result2)\n","else:\n","  print(\"something didnt go as expected, check if you solved the Task2!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OmKhIV4VlSeh"},"source":["**Note**: An important aspect is that we only augment the images used for training. So for testing we don't use the geometric augmentations."]},{"cell_type":"code","metadata":{"id":"WXV6_dOalSei"},"source":["# no augmentation for the test data only resizing, conversion to tensor and normalization\n","augmentation_test = transforms.Compose([\n","                    transforms.Resize((224,224)),\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(norm_mean, norm_std),\n","                    ])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uwzF3-r9lSet"},"source":["# Train, Test and Validation Split\n","It is a best practice to split the entire dataset into 3 parts:\n","- Train: Used to train a network.\n","- Validation: Fine tune the network.\n","- Test: Kept as unseen data to gauge the performance of out trained network.\n","\n","\n","The splitting should be done class wise so that we have equal representation of all classes in each subset of the data.\n","\n","![](https://github.com/IFL-CAMP/ML4MS_2020/blob/master/images/class_dist.png?raw=true)"]},{"cell_type":"code","metadata":{"id":"2s6SFpQClSeu"},"source":["import torch\n","from sklearn.model_selection import train_test_split\n","\n","classes = [ 'actinic keratoses', 'basal cell carcinoma', 'benign keratosis-like lesions', \n","           'dermatofibroma','melanoma', 'melanocytic nevi', 'vascular lesions']\n","\n","# get the total amount of images in the dataset\n","num_train = len(dataset)\n","\n","# create a list of indices for the whole dataset\n","indices = list(range(num_train))\n","\n","# get the class labels from the dataset object (0-6)\n","class_labels = dataset.targets\n","\n","# define the percentage of data that is not used for training\n","split_size = 0.2\n","\n","# call a function of sklarn that takes care of splitting the dataset into training and validation+testing\n","train_indices, test_indices, class_labels_train, class_labels_test = train_test_split(indices,\n","                                                                                       class_labels,\n","                                                                                       test_size=split_size,\n","                                                                                       shuffle=True,\n","                                                                                       stratify= class_labels,\n","                                                                                       random_state=42)\n","\n","# call a function of sklearn that splits validation+testing into validation and testing\n","train_indices, val_indices = train_test_split(train_indices,\n","                                               test_size=split_size,\n","                                               shuffle=True,\n","                                               stratify= class_labels_train,\n","                                               random_state=42)\n","\n","# Creating data samplers and loaders using the indices:\n","SubsetRandomSampler = torch.utils.data.sampler.SubsetRandomSampler\n","\n","# create instances of a torch class for picking random samples from our dataset\n","train_samples = SubsetRandomSampler(train_indices)\n","val_samples = SubsetRandomSampler(val_indices)\n","test_samples = SubsetRandomSampler(test_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_BUDZRBzlSe0"},"source":["# Dataloader\n","\n","We will now use the dataloader to load the entire dataset in small batches.\n","\n","**Epochs vs Iteration vs Batch size**\n","\n","One **Epoch** is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n","Now, we have more than 10000 images in our dataset. It is not possible to feed the entire dataset at once to the computer. So, we divide the data into several smaller batches.\n","\n","**Batch Size** is number of training examples present in a single step.\n","\n","**Iterations** are the number of batches needed to complete one epoch.\n","\n","An Example:\n","\n","If we have 10000 training images in our dataset. We can divide the dataset into **batches of 500** then it will take **20 iterations** to complete **1 epoch**.\n","\n","\n","That's where a pytorch dataloader is useful: https://pytorch.org/docs/stable/data.html"]},{"cell_type":"code","metadata":{"id":"vZv7MX3PlSe1"},"source":["# define the batch size for training, val and testing\n","batch_size, validation_batch_size, test_batch_size = 16, 16, 16\n","\n","# create and instance of a dataloader for training\n","train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False,num_workers=1, sampler= train_samples)\n","\n","# overwrite the dataset instance with the test augmentation (this is not nice code)\n","dataset = torchvision.datasets.ImageFolder(root= data_dir, transform=augmentation_test)\n","# create instances of a dataloaders for validation and testing\n","validation_data_loader = torch.utils.data.DataLoader(dataset, batch_size=validation_batch_size, shuffle=False, sampler=val_samples)\n","test_data_loader = torch.utils.data.DataLoader(dataset, batch_size=test_batch_size, shuffle=False, sampler=test_samples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5_oUPN9Z9PH"},"source":["print(f\"length train_indices train: {len(train_indices)}, number batches {len(train_data_loader)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgCr4IcX7tsp"},"source":["print(f\"length val_indices: {len(val_indices)}, number batches {len(validation_data_loader)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYqBEUFh7uDS"},"source":["print(f\"length test_indices: {len(test_indices)}, number batches {len(test_data_loader)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16ged-PV7ufH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFcYezPK9F61"},"source":["## Task 3 \n","\n","The Batchsize detemines the number of data that is used within one step. We noticed that the number of total batches for one training in the training is pretty large (401) our computer is able to process more than 16 (Batchsize) images at a time so we decide to reduce the total number of batches to a maximum of 50. \n","\n","Task: Reduce the total number of batches for the training from 401 to 50. "]},{"cell_type":"code","metadata":{"id":"N_hFs5j-97ZE"},"source":["# reduce number of batches from 401 to 50 \n","# hint: you can use the \"round\" command\n","\n","batch_size = 129\n","# create and instance of a dataloader for training\n","train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False,num_workers=1, sampler= train_samples)\n","\n","\n","print(f\"length train_indices train: {len(train_indices)}, number batches {len(train_data_loader)} , batch_size: {batch_size}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yV-HHuPv__1n"},"source":["come up with a formula on how to calculate the batchsize given a fixed number of total_batches and number_of_data"]},{"cell_type":"code","metadata":{"id":"wTIcJBtH_9X2"},"source":["# hint: total_batches = number_of_data / batchsize \n","import math\n","number_data = 6409\n","total_batches = 50\n","\n","def calc_batchsize(number_data, total_batches):\n","  batch_size = math.ceil(number_data / total_batches)\n","  return batch_size\n","\n","batch_size = calc_batchsize(number_data, total_batches)\n","print(f\"length train_indices train: {number_data}, number batches {total_batches} , batch_size: {batch_size}\")\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WxLeEYUmDxkJ"},"source":["Once your calc_batchsize function works you can submit your results - just execute the next cell"]},{"cell_type":"code","metadata":{"id":"Yl-KCoW-B58U"},"source":["import inspect\n","\n","if calc_batchsize(3209, 50) == 65:\n","  write_result(3, inspect.getsource(calc_batchsize))\n","else:\n","  print(\"something didnt go as expected, check if you solved the Task3!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mac_drJ5nUjP"},"source":["Let us display the loaded batched images:"]},{"cell_type":"code","metadata":{"id":"uwJzcQXXm_gz"},"source":["\n","\n","# functions to show an image\n","fig = plt.figure(figsize=(30, 30))\n","\n","def denorm(img):\n","    img[0,:,:] = (img[0,:,:] * np.asarray(norm_std[0])) + np.asarray(norm_mean[0])\n","    img[1,:,:] = (img[1,:,:] * np.asarray(norm_std[1])) + np.asarray(norm_mean[1])\n","    img[2,:,:] = (img[2,:,:] * np.asarray(norm_std[2])) + np.asarray(norm_mean[2])\n","    return img\n","\n","def imshow(img):\n","    img = denorm(img)\n","    npimg = img.numpy()\n","    plt.axis('off')\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","\n","# get first batch of training images\n","dataiter = iter(validation_data_loader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))\n","# print labels\n","print(' '.join('%5s, ' % classes[labels[j]] for j in range(len(labels))))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ei4V3lwyquFz"},"source":["Now, we have our dataset loaded Let's try to do something cool with it.\n","\n","For now, we will use an pre trained network to do inference on the test set of out data. Let's see how is the performance without training"]},{"cell_type":"code","metadata":{"id":"YBj-R8SprHm-"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","# load a pretrained model\n","from torch import nn\n","import torchvision\n","\n","num_classes = len(classes)\n","net = torchvision.models.resnet18(pretrained = True)\n","\n","# We replace last layer of resnet to match our number of classes which is 7\n","# more details next lecture\n","net.fc = nn.Linear(512, num_classes)\n","net = net.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRKNvPsFrCQ4"},"source":["# counter for correct predictions\n","correct = 0\n","# counter for all predicted samples\n","total = 0\n","\n","# set network to evaluation mode (next lecture)\n","net.eval()\n","\n","# this is for next lecture..\n","with torch.no_grad():\n","  dataiter = iter(train_data_loader)\n","  images, labels = dataiter.next()\n","  images, labels = images.to(device), labels.to(device)\n","  outputs = net(images)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","\n","print(\"finished ...\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LdHmfiqin5aS"},"source":["print(f\"labels: \\t{labels[:10]}\")\n","print(f\"predicted: \\t{predicted[:10]}\")\n","\n","print(f'Accuracy of the network on the test images: {(100 * correct / total)} %%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yRQvMJssioq0"},"source":["# Next time\n","\n","Now that we have the dataloaders and augmentations we can finally train our network so that it can actually learn to identify our melanomas."]}]}