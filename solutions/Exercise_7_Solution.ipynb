{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Exercise_7_Solution.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5yOkGTUF6Kp"
   },
   "source": [
    "# Setting up a network model and starting a first training\n",
    "\n",
    "In this exercise, we are going to practise, how to set up a neural network model and perform a first training with this network. For simplicity and fast processing, we are going to use the MNIST dataset directly obtainable from the torchvision package."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QzXxCHQb50En"
   },
   "source": [
    "# Imports\n",
    "# import libraries for simple image plotting and \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision as torchvision\n",
    "\n",
    "!pip install --upgrade -q gspread\n",
    "#!pip uninstall --yes 'google-auth==1.7.2'\n",
    "!pip install 'google-auth==1.16.1'\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "import gspread\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "tz = pytz.timezone('Europe/Berlin')\n",
    "\n",
    "gc = gspread.authorize(GoogleCredentials.get_application_default())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP9neANR56LG"
   },
   "source": [
    "The first time you run this there is probably an error. Just click on \"RUNTIME\" --> \"RESTART RUNTIME\" and run this cell again."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gM9BQxPI6Eef"
   },
   "source": [
    "student_name = \"yourName\"\n",
    "assert student_name != \"yourName\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSnRNgR68WQ-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are again using a form to monitor the progress for the different tasks:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3n5esauf6O7l",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#@title Result Form\n",
    "gsheet = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1oNM74xlAKuSJESKm2ggO5sduKYdVab4xzTShiG0lB34/edit?usp=sharing\")\n",
    "\n",
    "def write_result(task_number, result=None):\n",
    "  worksheet = gsheet.worksheet(\"task{}\".format(task_number))\n",
    "  current_time = datetime.datetime.now(tz).strftime(\"%X\")\n",
    "  current_date = str(datetime.date.today())\n",
    "  if result:\n",
    "    worksheet.append_row([student_name, current_time, current_date, result])\n",
    "    print(\"Task {} successfully solved by {} at {} with result: {}\".format(task_number, student_name, current_time, result))\n",
    "  else:\n",
    "    worksheet.append_row([student_name, current_time, current_date])\n",
    "    print(\"Task {} successfully solved by {} at {}\".format(task_number, student_name, current_time))\n",
    "\n",
    "print(\"Reporting enabled - write_result(number_of_task, result='your result') \")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JShCflCcOurF"
   },
   "source": [
    "Let us directly test the reporting:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I39Sd9NJOvj7"
   },
   "source": [
    "# Confirm that you are ready to go:\n",
    "write_result(0, 'Ready!!!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2I6F49b8fRx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If reporting is properly enabled, we can continue with the next steps.\n",
    "\n",
    "## Defining the augmentations\n",
    "\n",
    "As described in the last exercise, we now define the augmentations:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HjF-Aq9BF6K_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Imagenet values\n",
    "norm_mean = (0.4914)\n",
    "norm_std = (0.2023)\n",
    "\n",
    "# define the transformaitons the images go through each time it is used for training\n",
    "# includes augmentation AND normalization as described above\n",
    "augmentation_train = transforms.Compose([\n",
    "                                  # resize image to the network input size\n",
    "                                  transforms.Resize((28,28)),\n",
    "                                  # rotate the image with a angle from 0 to 60 (chosen randomly)\n",
    "                                  transforms.RandomRotation(degrees=10),\n",
    "                                  # convert the image into a tensor so it can be processed by the GPU\n",
    "                                  transforms.ToTensor(),\n",
    "                                  # normalize the image with the mean and std of ImageNet\n",
    "                                  transforms.Normalize(norm_mean, norm_std),\n",
    "                                   ])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yS7bZZuOF6LE"
   },
   "source": [
    "# no augmentation for the test data only resizing, conversion to tensor and normalization\n",
    "augmentation_test = transforms.Compose([\n",
    "                    transforms.Resize((28,28)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(norm_mean, norm_std),\n",
    "                    ])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmOkVMwTEQI-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we are creating an instance of our MNIST dataset as a torch class object. We can use an instance of the MNIST class in torchvision to easily load our data and control the training process. The dataset will be transferred into the dataloader objects later:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v194TjXLF6LH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\"\"\"\n",
    "Task 1: load the training part of the MNIST dataset that is provided by pytorch\n",
    "Here's an overview of datasets\n",
    "https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "Hints:\n",
    "- datasets can be found in torchvision.dataset.\n",
    "- root can just be your base folder e.g. \".\"\n",
    "- pick the test part for validation\n",
    "- don't forget to add the augmentations from above\n",
    "- make sure to enable the download\n",
    "\"\"\"\n",
    "\n",
    "import torchvision\n",
    "data_dir = \"/content/drive/My Drive/\"\n",
    "dataset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=augmentation_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jB0u2jdsF6LK"
   },
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get the total amount of images in the dataset\n",
    "num_train = len(dataset)\n",
    "\n",
    "# create a list of indices for the whole dataset\n",
    "indices = list(range(num_train))\n",
    "\n",
    "# get the class labels from the dataset object (0-6)\n",
    "class_labels = dataset.targets\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# define the percentage of data that is not used for training\n",
    "split_size = 0.2\n",
    "\n",
    "# call a function of sklarn that takes care of splitting the dataset into training and validation+testing\n",
    "train_indices, test_indices, class_labels_train, class_labels_test = train_test_split(indices,\n",
    "                                                                                       class_labels,\n",
    "                                                                                       test_size=split_size,\n",
    "                                                                                       shuffle=True,\n",
    "                                                                                       stratify= class_labels,\n",
    "                                                                                       random_state=42)\n",
    "\n",
    "# call a function of sklearn that splits validation+training into validation and training\n",
    "train_indices, val_indices = train_test_split(train_indices,\n",
    "                                               test_size=split_size,\n",
    "                                               shuffle=True,\n",
    "                                               stratify= class_labels_train,\n",
    "                                               random_state=42)\n",
    "\n",
    "# Creating data samplers and loaders using the indices:\n",
    "SubsetRandomSampler = torch.utils.data.sampler.SubsetRandomSampler\n",
    "\n",
    "# create instances of a torch class for picking random samples from our dataset\n",
    "train_samples = SubsetRandomSampler(train_indices)\n",
    "val_samples = SubsetRandomSampler(val_indices)\n",
    "test_samples = SubsetRandomSampler(test_indices)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFHp97ZTEsIj"
   },
   "source": [
    "In the next step, we transfer our dataset into the dataloader. The dataloader is another class in pytorch, which allows the control of the loading process during training. It autonatically generates batches of our training data and shuffles them. Like this, we can perform effective training for multiple epochs:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LW-_scRF6LQ"
   },
   "source": [
    "# define the batch size for training, val and testing\n",
    "batch_size, validation_batch_size, test_batch_size = 128, 128, 128\n",
    "\n",
    "# create and instance of a dataloader for training\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False,num_workers=1, sampler= train_samples)\n",
    "\n",
    "# overwrite the dataset instance with the test augmentation (this is not nice code)\n",
    "dataset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=augmentation_test)\n",
    "# create instances of a dataloaders for validation and testing\n",
    "validation_data_loader = torch.utils.data.DataLoader(dataset, batch_size=validation_batch_size, shuffle=False, sampler=val_samples)\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset, batch_size=test_batch_size, shuffle=False, sampler=test_samples)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "3QclrDwU6j2B"
   },
   "source": [
    "# Define a Convolutional Neural Network\n",
    "\n",
    "Pytorch makes it very easy to define a neural network. We have layers like Convolutions, ReLU non-linearity, Maxpooling etc. directly from torch library.\n",
    "\n",
    "In this tutorial, we use The LeNet architecture introduced by LeCun et al. in their 1998 paper, [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). As the name of the paper suggests, the authorsâ€™ implementation of LeNet was used primarily for OCR and character recognition in documents.\n",
    "\n",
    "The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "C4SswIxD6j2B"
   },
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_classes = len(classes)\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5,5)) \n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Hf87hhWCaqp"
   },
   "source": [
    "## Task 1\n",
    "Create a neural network capable of processing the image tensor 'img', which has only one channel. The network should contain at least one convolutional layer and one additional fully connected layer. Pay attention that within the fully connected layer the output dimension of the last convolutional layer has to fit the input dimension. Additionally, the output dimension of the fully connected layer before 'fc_fin' has to match the required input dimension of 'fc_fin'."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3ia2S8hrDCIL"
   },
   "source": [
    "img = torch.rand((1, 1, 200, 200))\n",
    "\n",
    "output_dim = 6\n",
    "\n",
    "class LeNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet2, self).__init__()\n",
    "        # --------------------\n",
    "        # Insert your layers here\n",
    "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5,5)) \n",
    "        self.fc1   = nn.Linear(16*48*48, 120)\n",
    "        self.fc2   = nn.Linear(120, 64)\n",
    "        # --------------------\n",
    "        self.fc_fin = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --------------------\n",
    "        # Insert your forward pass here\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # --------------------\n",
    "        x = self.fc_fin(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "model = LeNet2()\n",
    "\n",
    "task_done = False\n",
    "output = model(img)\n",
    "\n",
    "if output.size(1) == 6:\n",
    "  print('The correct output size has been generated.')\n",
    "  task_done = True\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aotjbdShGg17"
   },
   "source": [
    "When your model processes the image correctly, submit your result by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I5-gqjtwF9mH"
   },
   "source": [
    "if task_done:\n",
    "  write_result(1, str(model))\n",
    "else:\n",
    "  print(\"You didnt solve the task yet\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE_R_SSWF6Lb"
   },
   "source": [
    "# Define a Loss function\n",
    "\n",
    "Let's use a Classification Cross-Entropy loss.\n",
    "\n",
    "$H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)$\n",
    "\n",
    "### Median Frequency Balancing\n",
    "The MNIST dataset is fortunately a very balanced dataset containing almost equal numbers for every class. However, there are datasets like for example the HAM10000 dataset, which have a large imbalance in the amount of label occurrence. A prediction would be therefore biased towards stronger represented classes. As a solution, we use **Median Frequency Balancing**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vCJn-DV5F6Lc"
   },
   "source": [
    "# Median Frequency Balancing\n",
    "\n",
    "# get the class labels of each image\n",
    "class_labels = dataset.targets\n",
    "# empty array for counting instance of each class\n",
    "count_labels = np.zeros(len(classes))\n",
    "# empty array for weights of each class\n",
    "class_weights = np.zeros(len(classes))\n",
    "\n",
    "# populate the count array\n",
    "for l in class_labels:\n",
    "  count_labels[l] += 1\n",
    "\n",
    "# get median count\n",
    "median_freq = np.median(count_labels)\n",
    "\n",
    "# calculate the weigths\n",
    "for i in range(len(classes)):\n",
    "  class_weights[i] = median_freq/count_labels[i]\n",
    "\n",
    "# print the weights\n",
    "for i in range(len(classes)):\n",
    "    print(classes[i],\":\", class_weights[i])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pPUGMq3NCnP"
   },
   "source": [
    "Now we define the loss function with the weights"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j-Q72RJ-M84P"
   },
   "source": [
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRBdC31sF6Li"
   },
   "source": [
    "# Define the Optimizer\n",
    "\n",
    "The most common and effective Optimizer currently used is **Adam: Adaptive Moments**. You can look [here](https://arxiv.org/abs/1412.6980) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pmMjYnCjF6Lk"
   },
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = LeNet()\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "print(net)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oGr-AfqF6Ln"
   },
   "source": [
    "# Training\n",
    "\n",
    "After everything has been set up, we can now start an actual training on our MNIST dataset. To save time, for the moment we will run only ten epochs. Within the training, our dataloader is used to load a batch from our MNIST dataset. This batch is forwarded to the model. The corresponding output is compared against its labels with the chosen loss function, here called 'criterion'. Then, the loss values are backpropagated through the whole model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zyc3or5oF6Lo"
   },
   "source": [
    "num_epochs = 10\n",
    "\n",
    "def run_epoch():\n",
    "  running_loss = 0.0\n",
    "  data_loader = train_data_loader\n",
    "  for i, data in enumerate(data_loader):\n",
    "      # get the inputs\n",
    "      inputs, labels = data\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      # set the parameter gradients to zero\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = net(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      #compute accuracy\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      running_loss += loss.item()\n",
    "  return running_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = run_epoch()\n",
    "\n",
    "    running_loss /= len(train_data_loader)\n",
    "\n",
    "    print('Epoch: {}'.format(epoch+1))\n",
    "    print('Loss: {}' .format(running_loss))\n",
    "\n",
    "print('Finished Training')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yLHfbIu8Leka"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRfEARzBKd_U"
   },
   "source": [
    "## Task 2\n",
    "Rewrite the 'run_epoch' function, so that it can perform the training and validation in consecutive steps for each epoch. By using the corresponding dataloaders, the data can be easily provided to the model. The train() and eval() function change the model into the corresponding state. Please note that by running the previous code box the model is now already trained. We therefore re-initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ahiXcT1zLlpU"
   },
   "source": [
    "net = LeNet()\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "def run_epoch():\n",
    "  loss_dict = {'train': 0.0,\n",
    "               'val': 0.0}\n",
    "\n",
    "  # -----------------------\n",
    "  # Specifiy which dataloader needs to be used in a phase within a dictionary\n",
    "  data_loader = {'train': train_data_loader,\n",
    "                  'val': validation_data_loader}\n",
    "  # -----------------------\n",
    "\n",
    "  for phase in ['train', 'val']:\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "    num_all = 0\n",
    "\n",
    "    for i, data in enumerate(data_loader[phase]):\n",
    "\n",
    "      # get the inputs\n",
    "      inputs, labels = data\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      # set the parameter gradients to zero\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      if phase == 'train':\n",
    "        net.train()  # Set model to training mode\n",
    "      else:\n",
    "        net.eval()  # Set model to validation mode\n",
    "\n",
    "      # -------------------------------\n",
    "      # Calculate the network output and its loss\n",
    "      outputs = net(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      # -------------------------------\n",
    "\n",
    "      # The loss backpropagation and optimization step is only necessary when\n",
    "      # in training mode:\n",
    "\n",
    "      # -------------------------------\n",
    "      # Implement the loss backpropagation for the training phase only\n",
    "      if phase == 'train':\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      # -------------------------------\n",
    "      \n",
    "      #compute accuracy\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      running_loss += loss.item()\n",
    "      if phase == 'val':\n",
    "        num_correct += torch.sum(predicted == labels).item()\n",
    "        num_all += labels.size()[0]\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Save the correct loss for every phase within the dictionary\n",
    "    loss_dict[phase] = running_loss / len(data_loader[phase])\n",
    "    # ----------------------------------\n",
    "  accuracy = num_correct/num_all\n",
    "  return loss_dict, accuracy\n",
    "\n",
    "run_through = False\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    loss_dict, accuracy = run_epoch()\n",
    "\n",
    "    print('Epoch: {}'.format(epoch+1))\n",
    "    for phase in ['train', 'val']:\n",
    "      print('Loss {}: {}' .format(phase, loss_dict[phase]))\n",
    "      if phase == 'val':\n",
    "        print('Validation Accuracy: {}%' .format(np.round(accuracy, 3)*100))\n",
    "    print()\n",
    "    run_through = True\n",
    "\n",
    "print('Finished Training')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPv7urj9AddW"
   },
   "source": [
    "After you have sucessfully performed the training, please submit your results:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c3WP2f1mYIZ4"
   },
   "source": [
    "if run_through:\n",
    "  write_result(2, 'The last obtained validation loss is {}'.format(loss_dict['val']))\n",
    "else:\n",
    "  print(\"You didnt solve the task yet\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WTJyWOuAcwi"
   },
   "source": [
    "The following function is a useful tool to get information about your model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LwjnpkM5HDBO"
   },
   "source": [
    "from torchsummary import summary\n",
    "summary(net, input_size=(1, 28, 28))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIbJukXZA7ap"
   },
   "source": [
    "# Homework\n",
    "After performing the training and validation of your system you are now ready to perform the inference on your test set. Implement the inference step for the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lFoJ0C9OBYqY"
   },
   "source": [
    "running_loss = 0.0\n",
    "num_correct = 0\n",
    "num_all = 0\n",
    "data_loader = test_data_loader\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "  # ---------------------\n",
    "  # Implement the data loading and prediction on the test set\n",
    "  # get the inputs\n",
    "  inputs, labels = data\n",
    "  inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "  net.eval()  # Set model to validation mode\n",
    "\n",
    "  # -------------------------------\n",
    "  # Calculate the network output and its loss\n",
    "  outputs = net(inputs)\n",
    "  loss = criterion(outputs, labels)\n",
    "  # -------------------------------\n",
    "\n",
    "  # ---------------------\n",
    "  _, predicted = torch.max(outputs, 1)\n",
    "  num_correct += torch.sum(predicted == labels).item()\n",
    "  num_all += labels.size()[0]\n",
    "  running_loss += loss.item()\n",
    "\n",
    "running_loss /= len(data_loader)\n",
    "\n",
    "print('Loss for test set is {}'.format(running_loss))\n",
    "print('Test accuracy of the network: {}%'.format(np.round(num_correct/num_all, 3)*100))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujoTLsvfC8Vk"
   },
   "source": [
    "After you completed the inference, submit your test loss result here:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zZxDwplvCVUG"
   },
   "source": [
    "if not running_loss == 0.0:\n",
    "  write_result(3, 'The obtained test loss is {}'.format(running_loss))\n",
    "else:\n",
    "  print(\"You didnt solve the task yet\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}